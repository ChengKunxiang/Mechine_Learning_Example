# 机器学习示例
为了应付以后研究生工作中还会出现的机器学习问题，我写了一个库用于放置关于机器学习的示例。在这个库中，我放了各种各样的机器学习模型的示例。如果想用，可以直接将数据导入，得到对应的结果和图。

--------------

## 机器学习的学习笔记
下面介绍线性回归、XGBoost、梯度下降、随机森林等模型。

### XGBoost
<a href = "https://zhuanlan.zhihu.com/p/1909251307363078414"> 知乎传送门 </a>

### 梯度下降
<a href = "https://zhuanlan.zhihu.com/p/1893780224832954807"> 知乎传送门 </a>

### 随机森林
#### 决策树
![001](/doc/images/Random%20Forest/001.webp)
在解释随机森林前，需要先提一下决策树。决策树是一种很简单的算法，他的解释性强，也符合人类的直观思维。这是一种基于if-then-else规则的有监督学习算法，上面的图片可以直观的表达决策树的逻辑。

![002](/doc/images/Random%20Forest/002.webp)
随机森林是由很多决策树构成的，不同决策树之间没有关联。
当我们进行分类任务时，新的输入样本进入，就让森林中的每一棵决策树分别进行判断和分类，每个决策树会得到一个自己的分类结果，决策树的分类结果中哪一个分类最多，那么随机森林就会把这个结果当做最终的结果。

![003](/doc/images/Random%20Forest/003.webp)
1. 一个样本容量为N的样本，有放回的抽取N次，每次抽取1个，最终形成了N个样本。这选择好了的N个样本用来训练一个决策树，作为决策树根节点处的样本。
2. 当每个样本有M个属性时，在决策树的每个节点需要分裂时，随机从这M个属性中选取出m个属性，满足条件m << M。然后从这m个属性中采用某种策略（比如说信息增益）来选择1个属性作为该节点的分裂属性。
3. 决策树形成过程中每个节点都要按照步骤2来分裂（很容易理解，如果下一次该节点选出来的那一个属性是刚刚其父节点分裂时用过的属性，则该节点已经达到了叶子节点，无须继续分裂了）。一直到不能够再分裂为止。注意整个决策树形成过程中没有进行剪枝。
4. 按照步骤1~3建立大量的决策树，这样就构成了随机森林了。
------------------
## 进度
- [x] 完成线性回归、XGBoost、~~逻辑运算~~、梯度下降、随机森林等模型基本功能
- [x] 完成模型出图
- [x] 整合所有模型到一个代码（后称为集成）
- [x] 优化集成代码，整理输出文件